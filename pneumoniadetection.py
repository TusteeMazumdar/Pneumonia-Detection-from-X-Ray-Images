# -*- coding: utf-8 -*-
"""PneumoniaDetection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1W11Uzo8SR4BZWLLgxUl7U76EqNLvUp6o

## **Install necessary packages**
"""

!pip install opendatasets --upgrade

"""## **Import  necessary libraries**"""

from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout ,GlobalAveragePooling2D
from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img
from tensorflow.keras.models import Model, Sequential
from tensorflow.keras.applications import VGG16
from tensorflow.keras.utils import plot_model
from tensorflow.keras.optimizers import Adam
from torchvision.datasets import ImageFolder
from torchvision.transforms import ToTensor
from IPython.display import Image, display
from tensorflow.keras.metrics import Precision
from tensorflow.keras import layers
import matplotlib.pyplot as plt
from google.colab import files
from sklearn.metrics import confusion_matrix
import opendatasets as od
import tensorflow as tf
import numpy as np
import os

"""## **Download the dataset from Kaggle**"""

dataset_url="https://www.kaggle.com/datasets/paultimothymooney/chest-xray-pneumonia"
od.download(dataset_url)

"""## **Upload the kaggle.json file**"""

files.upload()

#Set up Kaggle username and key
!mkdir -p ~/.kaggle
!mv kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

#Download the dataset
!kaggle datasets download -d paultimothymooney/chest-xray-pneumonia

#Extract the dataset and unzip it
!unzip -q -o chest-xray-pneumonia.zip -x "__MACOSX/*"

# List folders
print(os.listdir('chest_xray'))

"""## **Define ImageDataGenerators and Directory paths**"""

data_dir = 'chest_xray'

# Define paths
train_dir = os.path.join(data_dir, 'train')
val_dir = os.path.join(data_dir, 'val')
test_dir = os.path.join(data_dir, 'test')

"""## **Creating ImageDataGenerators + Data Augmentation**"""

train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=30,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    validation_split=0.3
)

test_datagen = ImageDataGenerator(rescale=1./255)

# Create generators
train_generator = train_datagen.flow_from_directory(
    train_dir,
    target_size=(224, 224),
    batch_size=32,
    class_mode='binary',
    subset='training'
)

val_generator = train_datagen.flow_from_directory(
    train_dir,
    target_size=(224, 224),
    batch_size=32,
    class_mode='binary',
    subset='validation'
)

test_generator = test_datagen.flow_from_directory(
    test_dir,
    target_size=(224, 224),
    batch_size=32,
    class_mode='binary'
)

"""## **Finding the number of samples for each class in every sets (train, validation, test)**"""

# training set
train_normal = sum(train_generator.classes == 0)
train_pneumonia = sum(train_generator.classes == 1)

# validation set
val_normal = sum(val_generator.classes == 0)
val_pneumonia = sum(val_generator.classes == 1)

# test set
test_normal = sum(test_generator.classes == 0)
test_pneumonia = sum(test_generator.classes == 1)

print("Training set:")
print(f"Normal images: {train_normal}")
print(f"Pneumonia images: {train_pneumonia}")

print("\nValidation set:")
print(f"Normal images: {val_normal}")
print(f"Pneumonia images: {val_pneumonia}")

print("\nTest set:")
print(f"Normal images: {test_normal}")
print(f"Pneumonia images: {test_pneumonia}")

"""
## **Load dataset**"""

# Load the dataset
dataset = ImageFolder(train_dir, transform=ToTensor())
test_dataset = ImageFolder(test_dir, transform=ToTensor())
print('Size of raw/train dataset:', len(dataset))
print('Size of test dataset:', len(test_dataset))

"""## **Introducing our base model (VGG16) and fine-tuning it**"""

# Base model (VGG16)
base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

# Output shape of the base model
base_model_output_shape = base_model.output_shape
print("Base model output shape:", base_model_output_shape)

# Unfreeze layers of the base model for fine-tuning
base_model.trainable = True
fine_tune_at = 15

# Loop through the layers
for layer in base_model.layers[:-fine_tune_at]:
    layer.trainable = False

"""## **Transfer Learning**"""

# Appending custom classification head
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(128, activation='relu')(x)  # Add more dense layers if needed
predictions = Dense(1, activation='sigmoid')(x)  # Output layer

model = Model(inputs=base_model.input, outputs=predictions)

"""## **Setting Hyperparameters + model compilation**"""

# Setting learning rate
initial_learning_rate = 0.0001

# Using Adam Optimizer
optimizer = Adam(learning_rate=initial_learning_rate)

# Compile the model
model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.Precision(name='precision')])

"""## **Providing the summary of our proposed model architecture**"""

print("Model Summary:")
model.summary()

"""## **Pictorial presentation of our proposed model**"""

# Generate & save the model structure image
plot_model(model, to_file='model_architecture.png', show_shapes=True, show_layer_names=True)

# Display the structure
Image('model_architecture.png')

"""## **Callbacks for learning rate scheduling, early stopping, model checkpoint**"""

def scheduler(epoch, lr):
    if epoch < 15: # Constant learning rate for 1st 15 epochs
        return float(lr)
    else: # Learning rate decay after 15 epochs are done
        return float (lr * 0.9)

learning_rate_schedule = LearningRateScheduler(scheduler)
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
#checkpoint_cb = ModelCheckpoint("best_model.h5", save_best_only=True, monitor="val_accuracy", mode="max")
checkpoint_cb = ModelCheckpoint("best_model.keras", save_best_only=True, monitor="val_accuracy", mode="max")

"""## **Training of our Proposed Model**"""

history = model.fit(
    train_generator,
    steps_per_epoch=train_generator.samples // train_generator.batch_size,
    epochs=10,
    validation_data=val_generator,
    validation_steps=val_generator.samples // val_generator.batch_size,
    callbacks=[learning_rate_schedule, early_stopping, checkpoint_cb]
)

"""## **Further evaluation of our proposed model**"""

best_model = tf.keras.models.load_model("best_model.keras")
test_loss, test_acc, test_precision = best_model.evaluate(test_generator)
print(f'Test Accuracy: {test_acc}')
print(f'Test Loss: {test_loss}')
print(f'Test Precision: {test_precision}')

"""## **Calculating more significant Evaluation Metrices**"""

# Confusion matrix and metrics
y_true = test_generator.classes
y_pred = (best_model.predict(test_generator) > 0.5).astype("int32").flatten()

tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
recall = tp / (tp + fn)
f1_score = 2 * (test_precision * recall) / (test_precision + recall)

print(f'True Positives (TP): {tp}')
print(f'True Negatives (TN): {tn}')
print(f'False Positives (FP): {fp}')
print(f'False Negatives (FN): {fn}')
print(f'Recall: {recall * 100:.2f}%')
print(f'F1 Score: {f1_score * 100:.2f}%')

"""## **Accessing precision from history**"""

precision = history.history['precision']
val_precision = history.history['val_precision']

# Print the precision for each epoch
for epoch, (prec, val_prec) in enumerate(zip(precision, val_precision), 1):
    print(f'Epoch {epoch}: Precision = {prec}, Validation Precision = {val_prec}')

"""## **Display sample predictions**"""

def display_sample_predictions(generator, model, num_samples=5):
    x_batch, y_batch = next(generator)
    preds = model.predict(x_batch)
    plt.figure(figsize=(15, 10))

    for i in range(num_samples):
        plt.subplot(1, num_samples, i + 1)
        plt.imshow(x_batch[i])
        true_label = 'Pneumonia' if y_batch[i] else 'Normal'
        predicted_label = 'Pneumonia' if preds[i] > 0.5 else 'Normal'
        plt.title(f'True: {true_label}\nPred: {predicted_label}')
        plt.axis('off')

    plt.show()

# Sample predictions on test data
display_sample_predictions(test_generator, best_model)

predictions = model.predict(test_generator)
predicted_labels = ['Pneumonia' if pred > 0.5 else 'Normal' for pred in predictions]
print("Predicted labels:", predicted_labels)

"""## **Plotting Training Vs. Validation Accuracy Graph**"""

plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.ylim([0.85, 1.0])
plt.title('Training and Validation Accuracy')
plt.show()

"""## **Plotting Training Vs. Validation Loss Graph**"""

plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
epochs = len(history.history['loss'])
tick_values = range(1, epochs + 1)
plt.xticks(tick_values)
plt.title('Training and Validation Loss')
plt.show()

"""## **Plotting Training Vs. Validation Precision Graph**"""

plt.plot(history.history['precision'], label='Training Precision')
plt.plot(history.history['val_precision'], label='Validation Precision')
plt.xlabel('Epoch')
plt.ylabel('Precision')
plt.legend()
plt.ylim([0.90, 1.0])
plt.title('Training and Validation Precision')
plt.show()

"""## **Testing on an unseen image**"""

# Predict an unseen image
def load_and_preprocess_image(img_path, target_size=(224, 224)):
    img = load_img(img_path, target_size=target_size)
    img_array = np.expand_dims(img, axis=0)  # Add batch dimension
    img_array = img_array / 255.0  # Normalize to [0,1]
    return img_array

def predict_image(img_path):
    # Load and preprocess the image
    img_array = load_and_preprocess_image(img_path)

    # Prediction
    prediction = best_model.predict(img_array)
    predicted_class = int(prediction[0] > 0.5)

    # Cass values -> labels
    class_labels = {0: 'Normal', 1: 'Pneumonia'}
    predicted_label = class_labels[predicted_class]

    # Display image alongside prediction
    plt.imshow(load_img(img_path))
    plt.title(f'Predicted: {predicted_label}')
    plt.axis('off')
    plt.show()
    print(f"Predicted class label for the given image is: {predicted_class} ({predicted_label})")

# Upload an image & predict
uploaded = files.upload()

for img_path in uploaded.keys():
    predict_image(img_path)

"""## **Save the model**"""

model.save('pneumonia_detection_model.h5')